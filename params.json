{"name":"jiebaR","tagline":"R语言中文分词  Chinese text segmentation with R.    ","body":"# jiebaR\r\n\r\nLinux : [![Build Status](https://travis-ci.org/qinwf/jiebaR.svg?branch=master)](https://travis-ci.org/qinwf/jiebaR)　Mac : [![Build Status](https://travis-ci.org/qinwf/jiebaR.svg?branch=osx)](https://travis-ci.org/qinwf/jiebaR)　Windows : [![Build status](https://ci.appveyor.com/api/projects/status/k8swxpkue1caiiwi/branch/master?svg=true)](https://ci.appveyor.com/project/qinwf53234/jiebar/branch/master)\r\n\r\n[\"结巴\"中文分词]的R语言版本，支持最大概率法（Maximum Probability），隐式马尔科夫模型（Hidden Markov Model），索引模型（QuerySegment），混合模型（MixSegment），共四种分词模式，同时有词性标注，关键词提取，文本Simhash相似度比较等功能。项目使用了[Rcpp]和[CppJieba]进行开发。\r\n\r\n## 特性\r\n\r\n+ 支持 Windows，Linux，Mac 操作系统。\r\n+ 通过Rcpp Modules实现同时加载多个分词系统,可以分别使用不同的分词模式和词库。\r\n+ 支持多种分词模式、中文姓名识别、关键词提取、词性标注以及文本Simhash相似度比较等功能。\r\n+ 支持加载自定义用户词库，设置词频、词性。\r\n+ 同时支持简体中文、繁体中文分词。\r\n+ 支持自动判断编码模式。\r\n+ 比原[\"结巴\"中文分词]速度快，是其他R分词包的5-20倍。\r\n+ 安装简单，无需复杂设置。\r\n+ 可以通过[Rpy2]，[jvmr]等被其他语言调用。\r\n+ 基于MIT协议。\r\n\r\n## GitHub 版更新 v0.2.1\r\n\r\n+ 2X 分词速度\r\n+ 快速模式\r\n+ 修正特定环境下的编码转换问题\r\n\r\n## 安装\r\n\r\n通过CRAN安装:\r\n\r\n```r\r\ninstall.packages(\"jiebaR\")\r\nlibrary(\"jiebaR\")\r\n```\r\n\r\n同时还可以通过Github安装[开发版],建议使用 gcc >= 4.6 编译包：\r\n\r\n```r\r\nlibrary(devtools)\r\ninstall_github(\"qinwf/jiebaR\")\r\nlibrary(\"jiebaR\")\r\n```\r\n\r\n## 使用示例\r\n\r\n### 分词\r\n\r\njiebaR提供了四种分词模式，可以通过`worker()`来初始化分词引擎，使用`segment()`进行分词。\r\n\r\n```r\r\n##  接受默认参数，建立分词引擎 \r\nmixseg = worker()\r\n\r\n##  相当于：\r\n##       worker( type = \"mix\", dict = \"inst/dict/jieba.dict.utf8\",\r\n##               hmm  = \"inst/dict/hmm_model.utf8\",  ### HMM模型数据\r\n##               user = \"inst/dict/user.dict.utf8\") ### 用户自定义词库\r\n\r\nmixseg <= \"江州市长江大桥参加了长江大桥的通车仪式\"  ### <= 分词运算符\r\n\r\n## 相当于 segment( \"江州市长江大桥参加了长江大桥的通车仪式\" , mixseg )\r\n\r\n```\r\n\r\n```r\r\n[1] \"江州\"     \"市长\"     \"江大桥\"   \"参加\"     \"了\"       \"长江大桥\"\r\n[7] \"的\"       \"通车\"     \"仪式\" \r\n```\r\n\r\n支持对文件进行分词：\r\n\r\n```r\r\nmixseg <= \"./temp.dat\"  ### 自动判断输入文件编码模式，默认文件输出在同目录下。\r\n\r\n## segment( \"./temp.dat\" , mixseg )   \r\n```\r\n\r\n在加载分词引擎时，可以自定义词库路径，同时可以启动不同的引擎：\r\n\r\n最大概率法（MPSegment），负责根据Trie树构建有向无环图和进行动态规划算法，是分词算法的核心。\r\n\r\n隐式马尔科夫模型（HMMSegment）是根据基于人民日报等语料库构建的HMM模型来进行分词，主要算法思路是根据(B,E,M,S)四个状态来代表每个字的隐藏状态。 HMM模型由dict/hmm_model.utf8提供。分词算法即viterbi算法。\r\n\r\n混合模型（MixSegment）是四个分词引擎里面分词效果较好的类，结它合使用最大概率法和隐式马尔科夫模型。\r\n\r\n索引模型（QuerySegment）先使用混合模型进行切词，再对于切出来的较长的词，枚举句子中所有可能成词的情况，找出词库里存在。\r\n\r\n\r\n```r\r\nmixseg2 = worker(type  = \"mix\", dict = \"dict/jieba.dict.utf8\",\r\n                 hmm   = \"dict/hmm_model.utf8\",  \r\n                 user  = \"dict/test.dict.utf8\",\r\n                 detect=T,      symbol = F,\r\n                 lines = 1e+05, output = NULL\r\n                 ) \r\nmixseg2   ### 输出worker的设置\r\n```\r\n\r\n```r\r\nWorker Type:  Mix Segment\r\n\r\nDetect Encoding :  TRUE\r\nDefault Encoding:  UTF-8\r\nKeep Symbols    :  FALSE\r\nOutput Path     :  \r\nWrite File      :  TRUE\r\nMax Read Lines  :  1e+05\r\n\r\nFixed Model Components:  \r\n\r\n$dict\r\n[1] \"dict/jieba.dict.utf8\"\r\n\r\n$hmm\r\n[1] \"dict/hmm_model.utf8\"\r\n\r\n$user\r\n[1] \"dict/test.dict.utf8\"\r\n\r\n$detect $encoding $symbol $output $write $lines can be reset.\r\n```\r\n\r\n可以通过R语言常用的 `$`符号重设一些`worker`的参数设置，如 ` WorkerName$symbol = T `，在输出中保留标点符号。一些参数在初始化的时候已经确定，无法修改， 可以通过`WorkerName$PrivateVarible`来获得这些信息。\r\n\r\n```r\r\nmixseg$encoding\r\n\r\nmixseg$detect = F\r\n```\r\n\r\n可以自定义用户词库，推荐使用[深蓝词库转换]构建分词词库，它可以快速地将搜狗细胞词库等输入法词库转换为jiebaR的词库格式。\r\n\r\n```r\r\nShowDictPath()  ### 显示词典路径\r\nEditDict()      ### 编辑用户词典\r\n?EditDict()     ### 打开帮助系统\r\n```\r\n\r\n### 快速模式\r\n\r\n无需使用`worker()`，使用默认参数启动引擎，并立即进行分词：\r\n\r\n```r\r\nlibrary(jiebaR)\r\n\r\nqseg <= \"江州市长江大桥参加了长江大桥的通车仪式\" \r\n```\r\n\r\n```r\r\n[1] \"江州\"     \"市长\"     \"江大桥\"   \"参加\"     \"了\"       \"长江大桥\" \"的\"      \r\n[8] \"通车\"     \"仪式\"   \r\n```\r\n`qseg` ~ quick segmentation，使用默认分词模式，自动建立分词引擎，类似于`ggplot2`包里面的`qplot`函数。\r\n\r\n```r\r\n### 第一次运行时，启动默认引擎 quick_worker，第二次运行，不再启动引擎。\r\nqseg <= \"这是测试文本。\" \r\n\r\n```\r\n\r\n```r\r\n[1] \"这是\" \"测试\" \"文本\"\r\n```\r\n\r\n```r\r\n### 效果相同\r\nquick_worker <=  \"这是测试文本。\" \r\n\r\nqseg\r\n```\r\n\r\n```r\r\nWorker Type:  Mix Segment\r\n\r\nDetect Encoding :  TRUE\r\nDefault Encoding:  UTF-8\r\nKeep Symbols    :  FALSE\r\nOutput Path     :  NULL\r\n.......\r\n```\r\n\r\n可以通过`qseg$`重设模型参数，重设模型参数将会修改以后每次默认启动的默认参数，如果只是希望单次修改模型参数，可以使用非快速模式的修改方式`quick_worker$`。\r\n\r\n```r\r\nqseg$type = \"mp\" ### 重设模型参数的同时，重新启动引擎。\r\n\r\nqseg$type        ### 下次重新启动包是将使用现在的参数，构建模型。\r\n\r\nquick_worker$detect = T ### 临时修改，对下次重新启动包时，没有影响。\r\n\r\nget_qsegmodel()         ### 获得当前快速模式的默认参数\r\n\r\n```\r\n\r\n### 词性标注\r\n可以使用 `<=.tagger` 或者 `tag` 来进行分词和词性标注，词性标注使用混合模型模型分词，标注采用和 ictclas 兼容的标记法。\r\n\r\n```r\r\nwords = \"我爱北京天安门\"\r\ntagger = worker(\"tag\")\r\ntagger <= words\r\n```\r\n\r\n```r\r\n     r        v       ns       ns \r\n    \"我\"     \"爱\"   \"北京\" \"天安门\" \r\n```\r\n### 关键词提取\r\n关键词提取所使用逆向文件频率（IDF）文本语料库可以切换成自定义语料库的路径，使用方法与分词类似。`topn`参数为关键词的个数。\r\n\r\n```r\r\nkeys = worker(\"keywords\", topn = 1)\r\nkeys <= \"我爱北京天安门\"\r\nkeys <= \"一个文件路径.txt\"\r\n```\r\n```r\r\n  8.9954 \r\n\"天安门\" \r\n```\r\n### Simhash 与海明距离\r\n对中文文档计算出对应的simhash值。simhash是谷歌用来进行文本去重的算法，现在广泛应用在文本处理中。Simhash引擎先进行分词和关键词提取，后计算Simhash值和海明距离。\r\n\r\n```r\r\n words = \"hello world!\"\r\n simhasher = worker(\"simhash\",topn=2)\r\n simhasher <= \"江州市长江大桥参加了长江大桥的通车仪式\"\r\n```\r\n \r\n ```r\r\n$simhash\r\n[1] \"12882166450308878002\"\r\n\r\n$keyword\r\n   22.3853    8.69667 \r\n\"长江大桥\"     \"江州\" \r\n```\r\n\r\n```r\r\ndistance(\"江州市长江大桥参加了长江大桥的通车仪式\" , \"hello world!\", simhasher)\r\n\r\n```\r\n\r\n```r\r\n$distance\r\n[1] \"23\"\r\n\r\n$lhs\r\n   22.3853    8.69667 \r\n\"长江大桥\"     \"江州\" \r\n\r\n$rhs\r\n11.7392 11.7392 \r\n\"hello\" \"world\" \r\n```\r\n\r\n## 计划支持\r\n\r\n+ 支持 Windows , Linux , Mac 操作系统并行分词。\r\n+ 简单的自然语言统计分析功能。\r\n\r\n# jiebaR\r\n\r\nThis is a package for Chinese text segmentation, keyword extraction\r\nand speech tagging. `jiebaR` supports four\r\ntypes of segmentation modes: Maximum Probability, Hidden Markov Model, Query Segment and Mix Segment.\r\n\r\n## Features\r\n\r\n+ Support Windows, Linux,and Mac.\r\n+ Using Rcpp Modules to load different segmentation worker at the same time.\r\n+ Support Chinese text segmentation, keyword extraction, speech tagging and simhash computation.\r\n+ Custom dictionary path.\r\n+ Support simplified Chinese and traditional Chinese.\r\n+ New words identification.\r\n+ Auto encoding detection.\r\n+ Fast text segmentation.\r\n+ Easy installation.\r\n+ MIT license.\r\n\r\n## Installation\r\n\r\nInstall the latest development version from GitHub:\r\n\r\n```r\r\ndevtools::install_github(\"qinwf/jiebaR\")\r\n```\r\n\r\nInstall from [CRAN](http://cran.r-project.org/web/packages/jiebaR/index.html):\r\n\r\n```r\r\ninstall.packages(\"jiebaR\")\r\n```\r\n\r\n## Example\r\n\r\n### Text Segmentation\r\n\r\nThere are four segmentation models. You can use `worker()` to initialize a worker, and then use `<=` or `segment()` to do the segmentation.\r\n\r\n```r\r\nlibrary(jiebaR)\r\n\r\n##  Using default argument to initialize worker.\r\ncutter = worker()\r\n\r\n##       jiebar( type = \"mix\", dict = \"dictpath/jieba.dict.utf8\",\r\n##               hmm  = \"dictpath/hmm_model.utf8\",  ### HMM model data\r\n##               user = \"dictpath/user.dict.utf8\") ### user dictionary\r\n\r\n###  Note: Can not display Chinese character here.\r\n\r\ncutter <= \"This is a good day!\"  \r\n\r\n## OR segment( \"This is a good day!\" , cutter )\r\n\r\n```\r\n\r\n```r\r\n[1] \"This\" \"is\"   \"a\"    \"good\" \"day\" \r\n```\r\n\r\nYou can pipe a file path to cut file.\r\n\r\n```r\r\ncutter <= \"./temp.dat\"  ### Auto encoding detection.\r\n\r\n## OR segment( \"./temp.dat\" , cutter )   \r\n```\r\n\r\nThe package uses initialized engines for word segmentation. You\r\ncan initialize multiple engines simultaneously.\r\n\r\n```r\r\ncutter2 = worker(type  = \"mix\", dict = \"dict/jieba.dict.utf8\",\r\n                 hmm   = \"dict/hmm_model.utf8\",  \r\n                 user  = \"dict/test.dict.utf8\",\r\n                 detect=T,      symbol = F,\r\n                 lines = 1e+05, output = NULL\r\n                 ) \r\ncutter2   ### Print information of worker\r\n```\r\n\r\n```r\r\nWorker Type:  Mix Segment\r\n\r\nDetect Encoding :  TRUE\r\nDefault Encoding:  UTF-8\r\nKeep Symbols    :  FALSE\r\nOutput Path     :  \r\nWrite File      :  TRUE\r\nMax Read Lines  :  1e+05\r\n\r\nFixed Model Components:  \r\n\r\n$dict\r\n[1] \"dict/jieba.dict.utf8\"\r\n\r\n$hmm\r\n[1] \"dict/hmm_model.utf8\"\r\n\r\n$user\r\n[1] \"dict/test.dict.utf8\"\r\n\r\n$detect $encoding $symbol $output $write $lines can be reset.\r\n```\r\n\r\nThe model public settings can be modified and got using `$` , such as ` WorkerName$symbol = T `. Some private settings are fixed when the engine is initialized, and you can get them by `WorkerName$PrivateVarible`.\r\n\r\n```r\r\ncutter$encoding\r\n\r\ncutter$detect = F\r\n```\r\n\r\nUsers can specify their own custom dictionary to be included in the jiebaR default dictionary. jiebaR is able to identify new words, but adding your own new words can ensure a higher accuracy. [imewlconverter] is a good tools for dictionary construction.\r\n\r\n```r\r\nShowDictPath()  ### Show path\r\nEditDict()      ### Edit user dictionary\r\n?EditDict()     ### For more information\r\n```\r\n\r\n### Speech Tagging\r\nSpeech Tagging function `<=.tagger` or `tag` uses speech tagging worker to cut word and tags each word after segmentation, using labels compatible with ictclas.  `dict` `hmm` and `user` should be provided when initializing `jiebaR` worker.\r\n\r\n```r\r\nwords = \"hello world\"\r\ntagger = worker(\"tag\")\r\ntagger <= words\r\n```\r\n```r\r\n      x       x \r\n\"hello\" \"world\" \r\n```\r\n### Keyword Extraction\r\nKeyword Extraction worker use MixSegment model to cut word and use \r\n TF-IDF algorithm to find the keywords.  `dict`, `hmm`, \r\n `idf`, `stop_word` and `topn` should be provided when initializing  `jiebaR` worker.\r\n\r\n```r\r\nkeys = worker(\"keywords\", topn = 1)\r\nkeys <= \"words of fun\"\r\n```\r\n```r\r\n11.7392 \r\n  \"fun\" \r\n```\r\n### Simhash Distance\r\nSimhash worker can do keyword extraction and find \r\nthe keywords from two inputs, and then computes Hamming distance between them.\r\n\r\n```r\r\n words = \"hello world\"\r\n simhasher = worker(\"simhash\",topn=1)\r\n simhasher <= words\r\n ```\r\n \r\n ```r\r\n $simhash\r\n[1] \"3804341492420753273\"\r\n\r\n$keyword\r\n11.7392 \r\n\"hello\" \r\n```\r\n\r\n```r\r\ndistance(\"hello world\" , \"hello world!\" , simhasher)\r\n```\r\n\r\n```r\r\n$distance\r\n[1] \"0\"\r\n\r\n$lhs\r\n11.7392 \r\n\"hello\" \r\n\r\n$rhs\r\n11.7392 \r\n\"hello\" \r\n```\r\n\r\n## Future Development\r\n\r\n+ Support parallel programming on Windows , Linux , Mac.\r\n+ Simple Natural Language Processing features.\r\n\r\n## More Information and Issues\r\n[https://github.com/qinwf/jiebaR](https://github.com/qinwf/jiebaR)\r\n\r\n[https://github.com/aszxqw/cppjieba](https://github.com/aszxqw/cppjieba)\r\n\r\n[\"结巴\"中文分词]:https://github.com/fxsjy/jieba\r\n[Rcpp]:https://github.com/RcppCore/Rcpp\r\n[Cppjieba]:https://github.com/aszxqw/cppjieba\r\n[Rtools]:http://mirrors.xmu.edu.cn/CRAN/bin/windows/Rtools\r\n[深蓝词库转换]:https://github.com/studyzy/imewlconverter\r\n[开发版]:https://ci.appveyor.com/project/qinwf53234/jiebar/branch/master/artifacts\r\n[Rpy2]:http://rpy.sourceforge.net/\r\n[jvmr]:http://dahl.byu.edu/software/jvmr/\r\n[imewlconverter]:https://github.com/studyzy/imewlconverter","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}